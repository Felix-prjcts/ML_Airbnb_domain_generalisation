{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f54bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # ML Baseline Paris - Application des 7 M√©thodes du Cours\n",
    "# \n",
    "# **Objectif** : Tester les 7 m√©thodes sur le dataset Paris pr√©-trait√©\n",
    "# \n",
    "# Les donn√©es sont d√©j√† :\n",
    "# - Nettoy√©es et normalis√©es\n",
    "# - Encod√©es (one-hot, amenities, etc.)\n",
    "# - Avec target_class cr√©√©e (quartiles de prix)\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils import resample\n",
    "import umap.umap_ as umap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques import√©es\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Chargement des Donn√©es (D√©j√† pr√©-trait√©es)\n",
    "\n",
    "# %%\n",
    "# Chargement du dataset final (d√©j√† normalis√© et encod√©)\n",
    "df = pd.read_csv('dataset_paris_processed.csv.gz', compression='gzip')\n",
    "\n",
    "print(f\"üìä Dataset charg√© : {df.shape}\")\n",
    "print(f\"\\nüîç Aper√ßu des colonnes :\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# V√©rification target\n",
    "if 'target_class' in df.columns:\n",
    "    print(f\"\\n‚úÖ Target trouv√©e : {df['target_class'].nunique()} classes\")\n",
    "    print(df['target_class'].value_counts().sort_index())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Attention : 'target_class' non trouv√©e dans le dataset\")\n",
    "\n",
    "# Aper√ßu\n",
    "print(f\"\\nüìã Aper√ßu des 5 premi√®res lignes :\")\n",
    "print(df.head())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Pr√©paration X/y et Train/Test Split\n",
    "\n",
    "# %%\n",
    "print(\"=\" * 60)\n",
    "print(\"PR√âPARATION : S√©paration X/y et Train/Test Split\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# S√©paration X (features) et y (target)\n",
    "y = df['target_class']\n",
    "X = df.drop(columns=['target_class'])\n",
    "\n",
    "# Supprimer city_label si pr√©sent\n",
    "if 'city_label' in X.columns:\n",
    "    X = X.drop(columns=['city_label'])\n",
    "\n",
    "print(f\"\\nFeatures (X) : {X.shape[1]} colonnes\")\n",
    "print(f\"Target (y) : {len(y)} valeurs\")\n",
    "print(f\"\\nDistribution des classes :\")\n",
    "print(y.value_counts().sort_index())\n",
    "\n",
    "# Split 80/20 stratifi√©\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Split effectu√© :\")\n",
    "print(f\"  Train : {X_train.shape}\")\n",
    "print(f\"  Test  : {X_test.shape}\")\n",
    "print(f\"\\nDistribution Train :\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "print(f\"\\nDistribution Test :\")\n",
    "print(y_test.value_counts().sort_index())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. M√âTHODE 1 : PCA - Analyse Exploratoire\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"M√âTHODE 1 : PCA (Principal Component Analysis)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Standardisation pour PCA (donn√©es d√©j√† normalis√©es mais on re-standardise pour PCA)\n",
    "scaler_pca = StandardScaler()\n",
    "X_train_std = scaler_pca.fit_transform(X_train)\n",
    "X_test_std = scaler_pca.transform(X_test)\n",
    "\n",
    "# PCA compl√®te\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_train_std)\n",
    "\n",
    "# Variance expliqu√©e\n",
    "var_exp = pca_full.explained_variance_ratio_\n",
    "var_cum = np.cumsum(var_exp)\n",
    "\n",
    "# Nombre de composantes\n",
    "n_comp_95 = np.argmax(var_cum >= 0.95) + 1\n",
    "n_comp_90 = np.argmax(var_cum >= 0.90) + 1\n",
    "\n",
    "print(f\"\\nüìä R√©sultats PCA :\")\n",
    "print(f\"  ‚Ä¢ Composantes pour 90% variance : {n_comp_90}/{X.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Composantes pour 95% variance : {n_comp_95}/{X.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ R√©duction possible : {(1 - n_comp_95/X.shape[1])*100:.1f}%\")\n",
    "\n",
    "# Visualisations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Scree plot\n",
    "axes[0].bar(range(1, min(21, len(var_exp)+1)), var_exp[:20], alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title(\"Scree Plot - Variance par Composante\")\n",
    "axes[0].set_xlabel(\"Composante\")\n",
    "axes[0].set_ylabel(\"Variance Expliqu√©e\")\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Variance cumul√©e\n",
    "axes[1].plot(range(1, len(var_cum)+1), var_cum, marker='o', markersize=3)\n",
    "axes[1].axhline(y=0.95, color='r', linestyle='--', label='95%')\n",
    "axes[1].axhline(y=0.90, color='orange', linestyle='--', label='90%')\n",
    "axes[1].axvline(x=n_comp_95, color='r', linestyle=':', alpha=0.5)\n",
    "axes[1].set_title(\"Variance Cumul√©e\")\n",
    "axes[1].set_xlabel(\"Nombre de Composantes\")\n",
    "axes[1].set_ylabel(\"Variance Cumul√©e\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Projection 2D\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_train_pca_2d = pca_2d.fit_transform(X_train_std)\n",
    "scatter = axes[2].scatter(X_train_pca_2d[:, 0], X_train_pca_2d[:, 1], \n",
    "                         c=y_train, cmap='viridis', s=5, alpha=0.5)\n",
    "axes[2].set_title(f\"Projection 2D Train (var={pca_2d.explained_variance_ratio_.sum():.1%})\")\n",
    "axes[2].set_xlabel(f\"PC1 ({pca_2d.explained_variance_ratio_[0]:.1%})\")\n",
    "axes[2].set_ylabel(f\"PC2 ({pca_2d.explained_variance_ratio_[1]:.1%})\")\n",
    "plt.colorbar(scatter, ax=axes[2], label='Classe')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance PC1\n",
    "pc1_loadings = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'loading': np.abs(pca_full.components_[0])\n",
    "}).sort_values('loading', ascending=False)\n",
    "\n",
    "print(f\"\\nüìå Top 10 Features PC1 :\")\n",
    "print(pc1_loadings.head(10).to_string(index=False))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. M√âTHODE 2 : UMAP - Visualisation Non-Lin√©aire\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"M√âTHODE 2 : UMAP (Uniform Manifold Approximation)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# UMAP sur train (sous-√©chantillon si trop volumineux)\n",
    "sample_size = min(10000, len(X_train))\n",
    "if len(X_train) > sample_size:\n",
    "    print(f\"‚ö†Ô∏è Sous-√©chantillonnage pour UMAP : {sample_size} samples\")\n",
    "    indices = np.random.choice(len(X_train), sample_size, replace=False)\n",
    "    X_train_sample = X_train_std[indices]\n",
    "    y_train_sample = y_train.iloc[indices]\n",
    "else:\n",
    "    X_train_sample = X_train_std\n",
    "    y_train_sample = y_train\n",
    "\n",
    "print(\"üîÑ Calcul UMAP (30-60 secondes)...\")\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42, verbose=False)\n",
    "embedding_train = reducer.fit_transform(X_train_sample)\n",
    "\n",
    "# Visualisations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Color√© par classe\n",
    "colors_class = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c']\n",
    "for classe in range(4):\n",
    "    mask = y_train_sample == classe\n",
    "    axes[0].scatter(embedding_train[mask, 0], embedding_train[mask, 1], \n",
    "                   c=colors_class[classe], label=f'Classe {classe}', s=5, alpha=0.6)\n",
    "axes[0].set_title(\"UMAP Train - Color√© par Classe\")\n",
    "axes[0].set_xlabel(\"UMAP 1\")\n",
    "axes[0].set_ylabel(\"UMAP 2\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Densit√©\n",
    "axes[1].hexbin(embedding_train[:, 0], embedding_train[:, 1], gridsize=30, cmap='YlOrRd')\n",
    "axes[1].set_title(\"UMAP - Densit√©\")\n",
    "axes[1].set_xlabel(\"UMAP 1\")\n",
    "axes[1].set_ylabel(\"UMAP 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ UMAP r√©v√®le la structure non-lin√©aire des donn√©es\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. M√âTHODE 3 : LDA - Linear Discriminant Analysis\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"M√âTHODE 3 : LDA (Linear Discriminant Analysis)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# LDA (max 3 composantes pour 4 classes)\n",
    "lda = LinearDiscriminantAnalysis(n_components=3)\n",
    "X_train_lda = lda.fit_transform(X_train, y_train)\n",
    "X_test_lda = lda.transform(X_test)\n",
    "\n",
    "print(f\"Dimensions r√©duites : {X_train.shape[1]} ‚Üí {X_train_lda.shape[1]}\")\n",
    "print(f\"Variance expliqu√©e : {lda.explained_variance_ratio_}\")\n",
    "\n",
    "# Visualisation 2D\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for ax, (X_lda, y_set, title) in zip(axes, \n",
    "    [(X_train_lda, y_train, 'Train'), (X_test_lda, y_test, 'Test')]):\n",
    "    \n",
    "    for classe in range(4):\n",
    "        mask = y_set == classe\n",
    "        ax.scatter(X_lda[mask, 0], X_lda[mask, 1], \n",
    "                  c=colors_class[classe], label=f'Classe {classe}', alpha=0.5, s=10)\n",
    "    \n",
    "    ax.set_title(f\"LDA - {title}\")\n",
    "    ax.set_xlabel(f\"LD1 ({lda.explained_variance_ratio_[0]:.1%})\")\n",
    "    ax.set_ylabel(f\"LD2 ({lda.explained_variance_ratio_[1]:.1%})\")\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ LDA trouve les axes discriminants\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. M√âTHODE 4 : CART - Classification Tree\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"M√âTHODE 4 : CART (Classification Tree)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Arbre simple\n",
    "cart = DecisionTreeClassifier(\n",
    "    max_depth=5, \n",
    "    min_samples_split=100,\n",
    "    min_samples_leaf=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"üå≥ Entra√Ænement CART...\")\n",
    "cart.fit(X_train, y_train)\n",
    "\n",
    "# Pr√©dictions\n",
    "y_pred_cart = cart.predict(X_test)\n",
    "\n",
    "# M√©triques\n",
    "print(\"\\nüìä Performance CART :\")\n",
    "print(classification_report(y_test, y_pred_cart, \n",
    "                          target_names=['Bas', 'Moyen-Bas', 'Moyen-Haut', 'Haut'],\n",
    "                          zero_division=0))\n",
    "\n",
    "# Matrice de confusion\n",
    "cm_cart = confusion_matrix(y_test, y_pred_cart)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_cart, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=['Bas', 'MB', 'MH', 'Haut'],\n",
    "           yticklabels=['Bas', 'MB', 'MH', 'Haut'])\n",
    "plt.title(\"Matrice de Confusion - CART\")\n",
    "plt.ylabel(\"R√©el\")\n",
    "plt.xlabel(\"Pr√©dit\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance\n",
    "importance_cart = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': cart.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìå Top 15 Features CART :\")\n",
    "print(importance_cart.head(15).to_string(index=False))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "importance_cart.head(15).plot(kind='barh', x='feature', y='importance', legend=False)\n",
    "plt.title(\"Feature Importance - CART\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualisation arbre\n",
    "plt.figure(figsize=(25, 12))\n",
    "plot_tree(cart, feature_names=X.columns, class_names=['0','1','2','3'],\n",
    "         filled=True, fontsize=9, max_depth=3)\n",
    "plt.title(\"Arbre de D√©cision (3 niveaux)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "acc_cart = accuracy_score(y_test, y_pred_cart)\n",
    "print(f\"\\nüéØ Accuracy CART : {acc_cart:.4f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. M√âTHODE 5 : Bootstrap - Estimation de Variance\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"M√âTHODE 5 : Bootstrap (Estimation de Variance)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "n_iterations = 100\n",
    "scores_bootstrap = []\n",
    "\n",
    "print(f\"üîÑ {n_iterations} it√©rations bootstrap...\")\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    X_boot, y_boot = resample(X_train, y_train, random_state=i)\n",
    "    cart_boot = DecisionTreeClassifier(max_depth=5, min_samples_split=100, random_state=42)\n",
    "    cart_boot.fit(X_boot, y_boot)\n",
    "    y_pred = cart_boot.predict(X_test)\n",
    "    scores_bootstrap.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    if (i+1) % 20 == 0:\n",
    "        print(f\"  ‚úì {i+1}/{n_iterations}\")\n",
    "\n",
    "# Statistiques\n",
    "mean_acc = np.mean(scores_bootstrap)\n",
    "std_acc = np.std(scores_bootstrap)\n",
    "ci_low = np.percentile(scores_bootstrap, 2.5)\n",
    "ci_high = np.percentile(scores_bootstrap, 97.5)\n",
    "\n",
    "print(f\"\\nüìä R√©sultats Bootstrap :\")\n",
    "print(f\"  ‚Ä¢ Accuracy : {mean_acc:.4f} ¬± {std_acc:.4f}\")\n",
    "print(f\"  ‚Ä¢ IC 95%   : [{ci_low:.4f}, {ci_high:.4f}]\")\n",
    "print(f\"  ‚Ä¢ Min/Max  : {min(scores_bootstrap):.4f} / {max(scores_bootstrap):.4f}\")\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(scores_bootstrap, bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "plt.axvline(mean_acc, color='red', linestyle='--', linewidth=2, label=f'Moyenne: {mean_acc:.4f}')\n",
    "plt.axvline(ci_low, color='orange', linestyle=':', linewidth=2, label='IC 95%')\n",
    "plt.axvline(ci_high, color='orange', linestyle=':', linewidth=2)\n",
    "plt.title(f\"Distribution Bootstrap ({n_iterations} it√©rations)\")\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Fr√©quence\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Bootstrap estime la stabilit√© du mod√®le\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. M√âTHODE 6 : Bagging\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"M√âTHODE 6 : Bagging\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bagging = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=10),\n",
    "    n_estimators=50,\n",
    "    max_samples=0.8,\n",
    "    max_features=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"üîÑ Entra√Ænement Bagging (50 arbres)...\")\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging.predict(X_test)\n",
    "\n",
    "print(\"\\nüìä Performance Bagging :\")\n",
    "print(classification_report(y_test, y_pred_bagging,\n",
    "                          target_names=['Bas', 'Moyen-Bas', 'Moyen-Haut', 'Haut'],\n",
    "                          zero_division=0))\n",
    "\n",
    "cm_bagging = confusion_matrix(y_test, y_pred_bagging)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_bagging, annot=True, fmt='d', cmap='Greens',\n",
    "           xticklabels=['Bas', 'MB', 'MH', 'Haut'],\n",
    "           yticklabels=['Bas', 'MB', 'MH', 'Haut'])\n",
    "plt.title(\"Matrice de Confusion - Bagging\")\n",
    "plt.ylabel(\"R√©el\")\n",
    "plt.xlabel(\"Pr√©dit\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "acc_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "print(f\"\\nüéØ Accuracy Bagging : {acc_bagging:.4f}\")\n",
    "print(f\"üìà Gain vs CART : {(acc_bagging - acc_cart)*100:+.2f} points\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. M√âTHODE 7 : Random Forest\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"M√âTHODE 7 : Random Forest\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"üîÑ Entra√Ænement Random Forest (100 arbres)...\")\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"\\nüìä Performance Random Forest :\")\n",
    "print(classification_report(y_test, y_pred_rf,\n",
    "                          target_names=['Bas', 'Moyen-Bas', 'Moyen-Haut', 'Haut'],\n",
    "                          zero_division=0))\n",
    "\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Oranges',\n",
    "           xticklabels=['Bas', 'MB', 'MH', 'Haut'],\n",
    "           yticklabels=['Bas', 'MB', 'MH', 'Haut'])\n",
    "plt.title(\"Matrice de Confusion - Random Forest\")\n",
    "plt.ylabel(\"R√©el\")\n",
    "plt.xlabel(\"Pr√©dit\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance\n",
    "importance_rf = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìå Top 20 Features Random Forest :\")\n",
    "print(importance_rf.head(20).to_string(index=False))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "importance_rf.head(20).plot(kind='barh', x='feature', y='importance', legend=False, color='coral')\n",
    "plt.title(\"Feature Importance - Random Forest\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"\\nüéØ Accuracy Random Forest : {acc_rf:.4f}\")\n",
    "print(f\"üìà Gain vs CART : {(acc_rf - acc_cart)*100:+.2f} points\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10. Comparaison Finale\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARAISON FINALE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Mod√®le': ['CART', 'Bootstrap (CART)', 'Bagging', 'Random Forest'],\n",
    "    'Accuracy': [acc_cart, mean_acc, acc_bagging, acc_rf],\n",
    "    'Std': ['-', f'¬±{std_acc:.4f}', '-', '-'],\n",
    "    'Gain vs CART (%)': [0, (mean_acc - acc_cart)*100, (acc_bagging - acc_cart)*100, (acc_rf - acc_cart)*100]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + results.to_string(index=False))\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colors_models = ['#3498db', '#e74c3c', '#f39c12', '#2ecc71']\n",
    "bars = axes[0].bar(results['Mod√®le'], results['Accuracy'], color=colors_models, alpha=0.7, edgecolor='black')\n",
    "axes[0].axhline(y=0.25, color='gray', linestyle='--', alpha=0.5, label='Al√©atoire')\n",
    "axes[0].set_title(\"Comparaison des Performances\", fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=15, ha='right')\n",
    "\n",
    "for bar, acc in zip(bars, results['Accuracy']):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                f'{acc:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "axes[1].plot(results['Mod√®le'], results['Gain vs CART (%)'], marker='o', linewidth=2, markersize=10, color='green')\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[1].set_title(\"Gains vs CART\", fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel(\"Gain (%)\")\n",
    "axes[1].grid(alpha=0.3)\n",
    "plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=15, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüèÜ Meilleur mod√®le : {results.loc[results['Accuracy'].idxmax(), 'Mod√®le']}\")\n",
    "print(f\"‚úÖ Baseline Paris √©tablie !\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 11. Sauvegarde\n",
    "\n",
    "# %%\n",
    "results.to_csv('results_baseline_paris.csv', index=False)\n",
    "import joblib\n",
    "joblib.dump(rf, 'model_rf_paris.pkl')\n",
    "print(\"üíæ R√©sultats et mod√®le sauvegard√©s\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
