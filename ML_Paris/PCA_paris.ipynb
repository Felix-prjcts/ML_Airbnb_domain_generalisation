{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec65cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le dataset Paris préparé\n",
    "df = pd.read_csv('../data/paris_dataset_pre_ml.csv')\n",
    "\n",
    "print(\"Dataset chargé avec succès.\")\n",
    "print(f\"Dimensions : {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b753cb",
   "metadata": {},
   "source": [
    "# 0 - Sélection des variables numériques des données et les standardisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03b583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "var_num = df[num_cols].copy()\n",
    "\n",
    "# Imputation de la médiane pour la valeurs manquantes \n",
    "for col in var_num.columns:\n",
    "    if var_num[col].isna().any():\n",
    "        var_num[col] = var_num[col].fillna(var_num[col].median())\n",
    "\n",
    "# Standardisation\n",
    "scaler = StandardScaler()\n",
    "var_num_scaled = scaler.fit_transform(var_num)\n",
    "var_num_scaled_df = pd.DataFrame(var_num_scaled, columns=num_cols, index=df.index)\n",
    "\n",
    "\n",
    "print(\" (5 premières lignes des variables numériques standardisées) :\")\n",
    "print(var_num_scaled_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c917ab1",
   "metadata": {},
   "source": [
    "# 1 — Scree Plot : Variance expliquée\n",
    "\n",
    "Cette section affiche la proportion de variance expliquée par chaque composante principale et la variance cumulée.\n",
    "\n",
    "*Objectif : déterminer combien de composantes garder (ex. 80% / 90% de variance).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc353c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Scree plot — variance expliquée en pourcentage (composante cumulée + par composante)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "n_comp = 26\n",
    "print(f\"Scree: calcul PCA local avec {n_comp} composantes\")\n",
    "_pca_scree = PCA(n_components=n_comp)\n",
    "_pca_scree.fit(X_scaled_df)\n",
    "# Exprimer en pourcentage\n",
    "explained = _pca_scree.explained_variance_ratio_ * 100\n",
    "cum = explained.cumsum()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(range(1, len(explained)+1), explained, alpha=0.6, label='Pourcentage par PC')\n",
    "plt.plot(range(1, len(explained)+1), cum, marker='o', color='red', label='Variance cumulée (%)')\n",
    "plt.xlabel('Composante principale (PC)')\n",
    "plt.ylabel('Pourcentage de variance expliquée (%)')\n",
    "plt.title('Scree Plot — Variance expliquée par composante (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Indiquer le nombre de composantes pour atteindre 80% et 90% (en %)\n",
    "n80 = (cum >= 80).argmax() + 1 if (cum >= 80).any() else len(cum)\n",
    "\n",
    "print(f\"Composantes nécessaires pour 80% de variance: {n80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd83fd8c",
   "metadata": {},
   "source": [
    "Le graphique des valeurs propres montre une décroissance progressive et logarithmique de la variance expliquée, sans apparition d’un coude marqué. Cela indique que la variance est expliqué de manière diffuse entre les différentes composantes principales. Par conséquent, le choix du nombre de composantes retenues le pourcentage de variance cumulée expliquée dans notre cas on choisis de stopper a 80% soit 13 PC pour 26 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e6b578",
   "metadata": {},
   "source": [
    "# 2 — Valeurs propres (Eigenvalues)\n",
    "\n",
    "Ici on affiche les valeurs propres et un barplot des eigenvalues pour voir la distribution de la variance en valeurs (scree en valeurs).\n",
    "\n",
    "*Objectif : identifier les composantes ayant une variance significative.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Étude simple des valeurs propres (13 PC)\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "n_pc = 13\n",
    "pca13 = PCA(n_components=n_pc)\n",
    "pca13.fit(X_scaled_df)\n",
    "\n",
    "eigenvalues = pca13.explained_variance_\n",
    "explained_pct = (pca13.explained_variance_ratio_ * 100).round(3)\n",
    "cumulative_pct = explained_pct.cumsum().round(3)\n",
    "\n",
    "ev13 = pd.DataFrame({\n",
    "    'eigenvalue': eigenvalues.round(4),\n",
    "    'explained_pct': explained_pct,\n",
    "    'cumulative_pct': cumulative_pct\n",
    "}, index=[f'PC{i+1}' for i in range(n_pc)])\n",
    "\n",
    "print('\\n--- Valeurs propres (13 PC) ---')\n",
    "print(ev13)\n",
    "\n",
    "# Sauvegarde simple\n",
    "ev13.to_csv('../data/pca_ev_13_simple.csv')\n",
    "print('\\nSaved: ../data/pca_ev_13_simple.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a209cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Loadings pour PC1 — barplot des contributions (signées) et top features (sur 13 PC)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Récupère les loadings depuis la PCA utilisée ci-dessus (13 PC) ou fallback\n",
    "try:\n",
    "    components = _pca.components_\n",
    "    pc_cols = [f'PC{i+1}' for i in range(components.shape[0])]\n",
    "    loadings_df = pd.DataFrame(components.T, index=X_scaled_df.columns, columns=pc_cols)\n",
    "except Exception:\n",
    "    # fallback: recalcule avec 13 PC\n",
    "    _pca_fallback = PCA(n_components=min(13, X_scaled_df.shape[1]))\n",
    "    _pca_fallback.fit(X_scaled_df)\n",
    "    loadings_df = pd.DataFrame(_pca_fallback.components_.T, index=X_scaled_df.columns, columns=[f'PC{i+1}' for i in range(_pca_fallback.components_.shape[0])])\n",
    "\n",
    "# PC1 loadings\n",
    "pc1 = loadings_df['PC1']\n",
    "# Pour affichage horizontal, trier par valeur signée (négatifs d'abord)\n",
    "\n",
    "# Afficher les top N features par importance absolue\n",
    "top_n = 25\n",
    "top_feats = pc1.abs().sort_values(ascending=False).head(top_n).index\n",
    "# Obtenir valeurs signées pour ces features et trier pour le barh\n",
    "vals = pc1.loc[top_feats].sort_values(ascending=True)\n",
    "\n",
    "plt.figure(figsize=(8,10))\n",
    "plt.barh(vals.index, vals.values, color='C1', alpha=0.8)\n",
    "plt.xlabel('Loading (PC1) — contribution signée')\n",
    "plt.title(f'Loadings signés pour PC1 — top {top_n} features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTop {top_n} features par contribution absolue à PC1:\")\n",
    "print(pc1.abs().sort_values(ascending=False).head(top_n))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
