{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec65cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset chargé avec succès.\n",
      "Dimensions : (73111, 30)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#dataset preparation\n",
    "path = '../data/paris_dataset_final_ready.csv.gz'\n",
    "df = pd.read_csv(path, compression='gzip')\n",
    "\n",
    "print(\"Dataset chargé avec succès.\")\n",
    "print(f\"Dimensions : {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b753cb",
   "metadata": {},
   "source": [
    "# 0 - Sélection des variables numériques des données et les standardisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03b583e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (5 premières lignes des variables numériques standardisées) :\n",
      "   accommodates  bedrooms      beds  minimum_nights_avg_ntm  \\\n",
      "0     -0.814720 -0.353928 -1.220511               -0.211579   \n",
      "1     -1.408411 -0.353928 -0.440149                0.515598   \n",
      "2     -0.814720 -1.427401 -0.440149               -0.237549   \n",
      "3      0.372662  0.719544 -0.440149               -0.003814   \n",
      "4     -1.408411 -0.353928 -0.440149               -0.159638   \n",
      "\n",
      "   number_of_reviews  price_clean  dist_to_center  bathrooms_qty    has_ac  \\\n",
      "0          -0.325300     0.015515        0.319210      -0.359003 -0.413491   \n",
      "1           0.347356    -0.767846       -0.974245       0.574525 -0.413491   \n",
      "2           6.737593    -0.619420       -1.568968      -0.359003 -0.413491   \n",
      "3           5.870220    -0.355551       -1.830771      -0.359003 -0.413491   \n",
      "4           4.524907    -0.817321       -0.110272      -0.359003  2.418430   \n",
      "\n",
      "   has_elevator  ...  has_washing_machine  standing_thermal  \\\n",
      "0     -0.664876  ...             0.433016         -0.659423   \n",
      "1     -0.664876  ...             0.433016         -0.659423   \n",
      "2     -0.664876  ...             0.433016          0.699132   \n",
      "3     -0.664876  ...             0.433016         -0.659423   \n",
      "4      1.504040  ...            -2.309382          2.057688   \n",
      "\n",
      "   score_chef_kitchen  score_coffee  score_wellness  score_work  score_view  \\\n",
      "0           -1.363121     -0.988409       -0.554036   -0.794123   -0.357655   \n",
      "1            0.318842     -0.007700       -0.554036   -0.794123   -0.357655   \n",
      "2            0.879497      0.973010       -0.554036    0.963912   -0.357655   \n",
      "3            0.318842     -0.007700       -0.554036    0.963912   -0.357655   \n",
      "4           -0.241812     -0.007700       -0.554036   -0.794123   -0.357655   \n",
      "\n",
      "   score_secure  neigh_popularity  target_class  \n",
      "0      0.666523         -0.934173      0.453920  \n",
      "1     -0.576371         -1.176645     -1.319434  \n",
      "2     -0.576371         -1.177752     -1.319434  \n",
      "3     -0.576371         -1.177752     -0.432757  \n",
      "4     -0.576371          1.864779     -1.319434  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "var_num = df[num_cols].copy()\n",
    "\n",
    "#impute missing values with the median\n",
    "for col in var_num.columns:\n",
    "    if var_num[col].isna().any():\n",
    "        var_num[col] = var_num[col].fillna(var_num[col].median())\n",
    "\n",
    "#standardization\n",
    "scaler = StandardScaler()\n",
    "var_num_scaled = scaler.fit_transform(var_num)\n",
    "var_num_scaled_df = pd.DataFrame(var_num_scaled, columns=num_cols, index=df.index)\n",
    "\n",
    "\n",
    "print(\"First 5 rows:\")\n",
    "print(var_num_scaled_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c917ab1",
   "metadata": {},
   "source": [
    "# 1 — Scree Plot : Variance expliquée\n",
    "\n",
    "Cette section affiche la proportion de variance expliquée par chaque composante principale et la variance cumulée.\n",
    "\n",
    "*Objectif : déterminer combien de composantes garder (ex. 80% / 90% de variance).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc353c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scree: calcul PCA local avec 26 composantes\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_scaled_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScree: calcul PCA local avec \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_comp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m composantes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m _pca_scree = PCA(n_components=n_comp)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m _pca_scree.fit(\u001b[43mX_scaled_df\u001b[49m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Exprimer en pourcentage\u001b[39;00m\n\u001b[32m     13\u001b[39m explained = _pca_scree.explained_variance_ratio_ * \u001b[32m100\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'X_scaled_df' is not defined"
     ]
    }
   ],
   "source": [
    "#scree plot — explained variance percentage (cumulative + per component)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "n_comp = 26\n",
    "print(f\"Scree: local PCA calculation with {n_comp} components\")\n",
    "_pca_scree = PCA(n_components=n_comp)\n",
    "_pca_scree.fit(X_scaled_df)\n",
    "#express as percentage\n",
    "explained = _pca_scree.explained_variance_ratio_ * 100\n",
    "cum = explained.cumsum()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(range(1, len(explained)+1), explained, alpha=0.6, label='Percentage per PC')\n",
    "plt.plot(range(1, len(explained)+1), cum, marker='o', color='red', label='Cumulative variance (%)')\n",
    "plt.xlabel('Principal component (PC)')\n",
    "plt.ylabel('Percentage of explained variance (%)')\n",
    "plt.title('Scree Plot — Explained variance per component (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#indicate number of components needed to reach 80% and 90% variance\n",
    "n80 = (cum >= 80).argmax() + 1 if (cum >= 80).any() else len(cum)\n",
    "\n",
    "print(f\"Components needed for 80% variance: {n80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd83fd8c",
   "metadata": {},
   "source": [
    "Le graphique des valeurs propres montre une décroissance progressive et logarithmique de la variance expliquée, sans apparition d’un coude marqué. Cela indique que la variance est expliqué de manière diffuse entre les différentes composantes principales. Par conséquent, le choix du nombre de composantes retenues le pourcentage de variance cumulée expliquée dans notre cas on choisis de stopper a 80% soit 13 PC pour 26 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e6b578",
   "metadata": {},
   "source": [
    "# 2 — Valeurs propres (Eigenvalues)\n",
    "\n",
    "Ici on affiche les valeurs propres et un barplot des eigenvalues pour voir la distribution de la variance en valeurs (scree en valeurs).\n",
    "\n",
    "*Objectif : identifier les composantes ayant une variance significative.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple eigenvalue study (13 pcs)\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "n_pc = 13\n",
    "pca13 = PCA(n_components=n_pc)\n",
    "pca13.fit(X_scaled_df)\n",
    "\n",
    "eigenvalues = pca13.explained_variance_\n",
    "explained_pct = (pca13.explained_variance_ratio_ * 100).round(3)\n",
    "cumulative_pct = explained_pct.cumsum().round(3)\n",
    "\n",
    "ev13 = pd.DataFrame({\n",
    "    'eigenvalue': eigenvalues.round(4),\n",
    "    'explained_pct': explained_pct,\n",
    "    'cumulative_pct': cumulative_pct\n",
    "}, index=[f'PC{i+1}' for i in range(n_pc)])\n",
    "\n",
    "print('\\n--- Eigenvalues (13 PCs) ---')\n",
    "print(ev13)\n",
    "\n",
    "#simple save\n",
    "ev13.to_csv('../data/pca_ev_13_simple.csv')\n",
    "print('\\nSaved: ../data/pca_ev_13_simple.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a209cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loadings for pc1 — barplot of signed contributions and top features (over 13 pcs)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#retrieve loadings from the PCA used above (13 pcs) or fallback\n",
    "try:\n",
    "    components = _pca.components_\n",
    "    pc_cols = [f'PC{i+1}' for i in range(components.shape[0])]\n",
    "    loadings_df = pd.DataFrame(components.T, index=X_scaled_df.columns, columns=pc_cols)\n",
    "except Exception:\n",
    "    #fallback: recompute with 13 pcs\n",
    "    _pca_fallback = PCA(n_components=min(13, X_scaled_df.shape[1]))\n",
    "    _pca_fallback.fit(X_scaled_df)\n",
    "    loadings_df = pd.DataFrame(_pca_fallback.components_.T, index=X_scaled_df.columns, columns=[f'PC{i+1}' for i in range(_pca_fallback.components_.shape[0])])\n",
    "\n",
    "#pc1 loadings\n",
    "pc1 = loadings_df['PC1']\n",
    "#for horizontal display, sort by signed value (negatives first)\n",
    "\n",
    "#display top n features by absolute importance\n",
    "top_n = 25\n",
    "top_feats = pc1.abs().sort_values(ascending=False).head(top_n).index\n",
    "#obtain signed values for these features and sort for the barh\n",
    "vals = pc1.loc[top_feats].sort_values(ascending=True)\n",
    "\n",
    "plt.figure(figsize=(8,10))\n",
    "plt.barh(vals.index, vals.values, color='C1', alpha=0.8)\n",
    "plt.xlabel('Loading (PC1) — signed contribution')\n",
    "plt.title(f'Signed loadings for PC1 — top {top_n} features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTop {top_n} features by absolute contribution to PC1:\")\n",
    "print(pc1.abs().sort_values(ascending=False).head(top_n))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envproject (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
